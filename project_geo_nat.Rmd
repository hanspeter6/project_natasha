---
title: "Soil Characteristics and Burn"
author: "Hans-Peter Bakker for Natasha Moore"
date: "6/1/2018"
output:
  pdf_document: default
  html_document: default
  word_document: default
bibliography: natasha.bib
---
---
nocite: |
        @rpart, @rpart.plot, @dplyr, @FactoMineR, @factoextra, @caret, @brglm, @corrplot, @mice, @glmnet, @RStudio, bestglm
...


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r}
# libraries:
library(dplyr)
library(FactoMineR)
library(factoextra)
library(rpart)
library(rpart.plot)
library(caret)
# library(logistf)
library(brglm)
library(corrplot)
library(mice)
library(bestglm)
```

```{r}
# read .csv file:
geo_data <- read.table("trial2.csv",
                       header = TRUE, sep = ";")
```
##Data Cleaning and Initial Exploration

After first abbreviating or shortening some of variable names and some of the category labels, the provided Excel spreadsheet was read in as a .csv file.

The variables *site*, *type* and *burn* were converted to factor variables.

A summary of the variables is shown below and reflects missing values for *hsv* and for *hp*.

```{r}
#change site, type, burn to factor variables
geo_data$site <- factor(geo_data$site)
geo_data$type <- factor(geo_data$type)
geo_data$burn <- factor(geo_data$burn, labels = c("No Burn", "Burn"))

summary(geo_data)
```

```{r}
# numeric dataset
geo_data_nums <- data.frame(scale(geo_data[,c('elevation',
                                              'slope',
                                              'hsv', # 3 NA's
                                              'hp',
                                              'moisture',
                                              'ph',
                                              'loss_ig',
                                              'sand',
                                              # 'silt', # due to singularity
                                              'clay')]))

```

The missing values were replaced by imputed values that were based on the medians of the respective variables. (GET BACK TO THIS!!)

```{r}
# impute missing values: multivariate imputations by chained equations (mice)
# which rows (hp row 35; hsv rows 35,64 & 79 )
imputed <- mice(geo_data, print = FALSE)
# imputed$imp$hsv
# imputed$imp$hp
#pool(imputed)
```

Finally, given the linear dependence between between *silt*, *clay* and *sand* (they add up to 100%), *silt* was dropped from the dataset.

### Correlation Matrix

A plot indicating the correlations between the numeric variables is shown below (and a .jpeg file for publication was generated).

```{r}
# consider correlations
cor_mat <- cor(geo_data_nums, use = 'pairwise.complete.obs')
corrplot(cor_mat, method = 'pie', tl.col = "black")
jpeg("corrplot1_geo.jpeg")
corrplot(cor_mat, method = 'pie', tl.col = "black")
dev.off()
```

The strong correlation between *moisture* and *loss_ig* would suggest that only one of these is required. The relatively strong negative correlation between *sand* and *clay* can also be expected due to the fact that a high percentage of the one automatically implies a low percentage of the other.

The generally high levels of correlation would suggest that dimension reduction may be worth considering, although it may  be accompanied by more difficult interpretations.

### Principal Components Analysis

To examine the latent dimensionality of the numerical data, principal components analysis was done on the standardised numeric variables. The scree plot below suggests an *elbow* at four components. The summary output shows that a cumulative 86% of the variance would be captured by the first four components. (a .jpeg file of the screeplot is also generated)

```{r}
# to explore dimensionality of numerical variables for now only complete cases
pca_geo <- princomp(geo_data_nums[complete.cases(geo_data_nums),])
screeplot(pca_geo, type = "lines", main = "Scree Plot of Principal Components", npcs = 8)
abline(v = 4, lty = 2)
# for printing:
jpeg("pca_screeplot.jpeg")
screeplot(pca_geo, type = "lines", main = "Scree Plot of Principal Components", npcs = 8)
abline(v = 4, lty = 2)
dev.off()

summary(pca_geo)
```

The vector plots below (which are also reproduced in .jpeg files) give some idea of the contributions of variables on the four dimensions. These could be used to interpret the meaning of the components/factors.

Although this approach may be interesting as a point of discussion on how the variables relate to each other, taking this approach further would more likely lead to greater difficulties in the interpretation of outcomes and effects, but if required, this approach of working with latent constructs can be pursued.

```{r}
# using factomineR
pca_geo_factoMineR <- PCA(geo_data_nums[complete.cases(geo_data_nums),], ncp = 4, graph = FALSE)

fviz_pca_var(pca_geo_factoMineR,
             axes = c(1,2),
             col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             title = "Dimensions 1 & 2",
             repel = TRUE # Avoid text overlapping
)

jpeg("contributions_pca_1n2.jpeg")
fviz_pca_var(pca_geo_factoMineR,
             axes = c(1,2),
             col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             title = "Dimensions 1 & 2",
             repel = TRUE # Avoid text overlapping
)
dev.off()

fviz_pca_var(pca_geo_factoMineR,
             axes = c(3,4),
             col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             title = "Dimensions 3 & 4",
             repel = TRUE # Avoid text overlapping
)

jpeg("contributions_pca_3n4.jpeg")
fviz_pca_var(pca_geo_factoMineR,
             axes = c(3,4),
             col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             title = "Dimensions 3 & 4",
             repel = TRUE # Avoid text overlapping
)
dev.off()

```
## Partition Tree

To offer some initial exploration of the link between the variables and the outcome (*Burn* or *No Burn*), a simple partition exercise (or *tree*) was considered with all the variables - numeric and categorical - as response variables. Only the adjusted (reduced levels) categorical variables were used. Two 

plots showing the first four levels are shown below. (and also added as a further .jpeg file for use in publication). In the second plot, the variable *colour* was removed.

```{r}
# consider classification tree for explanatory reasons:
tree_geo1 <- rpart(burn ~ 
                          area2 +
                          geology2 +
                          aspect2 +
                          veg +
                          elevation +
                          slope +
                          hsv +
                          hp +
                          moisture +
                          ph +
                          loss_ig +
                          colour +
                          sand +
                          silt +
                          clay
                          ,
                  data = geo_data)

tree_geo2 <- rpart(burn ~ 
                          area2 +
                          geology2 +
                          aspect2 +
                          veg +
                          elevation +
                          slope +
                          hsv +
                          hp +
                          moisture +
                          ph +
                          loss_ig +
                          # colour +
                          sand +
                          silt +
                          clay
                          ,
                  data = geo_data)
```

```{r}

# all variables
rpart.plot(tree_geo1, type = 4, extra = 1, cex = 0.8, main = "Burn Partition: All Variables")

jpeg("partition_tree1.jpeg")
rpart.plot(tree_geo1, type = 4, extra = 1, cex = 0.8, main = "Burn Partition: All Variables")
dev.off()

# minus 'colour'
rpart.plot(tree_geo2, type = 4, extra = 1, cex = 0.8, main = "Burn Partition: Minus *colour*")

jpeg("partition_tree2.jpeg")
rpart.plot(tree_geo2, type = 4, extra = 1, cex = 0.8, main = "Burn Partition: Minus *colour*")
dev.off()

```

The predictive power of the second partitioning model (without *colour*), which showed higher levels of accuracy than the full set, using the existing data as predictors suggests relatively high levels of accuracy - as can be seen in the confusion matrix and relevant statistics shown below. If the purpose of this project was prediction, this line of investigation could be pursued into considering random forests and other methods.

```{r}
# consider prediction power of single tree:
pred_class <- predict(tree_geo2, type = "class")
confusionMatrix(data = pred_class, reference = geo_data$burn)
```

## Logistic Regression 

Given the binomial nature of the outcome variable, logistic regression was considered to quantify the relative importance of predictor variables on the class of outcome.

Initial attempts were thwarted due to the fact that the data appeared to be perfectly separable. This meant that the standard estimating methods (Fisher's Scoring) could not be applied. An examination of the data indicated that various combinations of the variables were responsible for the separation and as a result it was not possible to reconstitute or reconsider one or two of the variables only. An alternative, *bias reduction*, method proposed by Firth (1993) and cited in @brglm was adopted. Although this method may offer only second-order unbiased results, the estimates are finite and compare well with more established estimating procedures.

Given the number of variables it was possible to run iterations of all the combinations of explanatory variables to identify the top five subset selections.

```{r cache = TRUE}
xy_frame <- geo_data %>%
        select( area2,
                geology2,
                aspect2,
                veg,
                elevation,
                slope,
                hsv ,
                hp,
                moisture,
                ph,
                loss_ig,
                colour,
                sand,
                clay,
                y = burn)
best_set_bic <- bestglm(xy_frame, family = binomial, IC = "BIC", method = "exhaustive")
# best_set_bicq <- bestglm(xy_frame, family = binomial, IC = "BICq", q = 0.25, method = "exhaustive")
```

The five "best" models based on the lowest Bayesian Information Criterion (BIC) are reflected below.

```{r}
best_set_bic$BestModels
#summary(best_set_bic$BestModel) #can use this to build model using brglm()
```

The following variables appear consistently in the top three and would therefore be included in any model: *elevation*, *hsv*, *hp*, *ph* and *sand*.

The third model, which includes both *aspect* and *veg*, but not *colour* will be used in the estimation based on advice from the researcher.

The list of variables that were included in the estimation of the final model will therefore be:

* *aspect*
* *veg*
* *elevation*
* *hsv*
* *hp*
* *ph*
* *loss_ig*
* *sand*

```{r}
model_geo <- brglm(burn ~
                         aspect2 +
                         veg +
                         elevation +
                         hsv +
                         hp +
                         ph +
                         loss_ig +
                         sand,
                 data = geo_data,
                 family = "binomial")

```

To consider the model fit its *deviance*, which in logistic regression can be considered to exhibit a Chi-Squared ($\chi^2$) distribution, and the model's degrees of freedom were used to test for significance. A p-value of close one and therefore considerably larger than an 0.05 cut-off level indicated an acceptable fit (@farawayExt).

Additionally, the squared correlation between model predicted and observed outcomes was considered and, given a value of `r round(cor(ifelse(predict(model_geo, type = "response") > 0.5, 1, 0), ifelse(geo_data$burn[-c(35,64,79)] == "Burn", 1, 0))^2, 4) `, further suggests a reasonable result.

Finally, as with the partition tree above, a confusion matrix, shown below, was drawn from the observed and predicted values, also indicating a model with strong predictive power.

```{r}
#  `r round(pchisq(deviance(model_geo), df.residual(model_geo), lower = FALSE))`
# # consider observed vs fitted values: and compute the pearson correlation 'r' . Squaring it gives good idea..
preds <- predict(model_geo, type = "response")
# preds_num <- ifelse(predict(model_geo, type = "response") > 0.5, 1, 0)
# obs_num <- ifelse(geo_data$burn[-c(35,64,79)] == "Burn", 1, 0)
# round(cor(ifelse(predict(model_geo, type = "response") > 0.5, 1, 0), ifelse(geo_data$burn[-c(35,64,79)] == "Burn", 1, 0))^2, 4)

# confusion matrix
preds_cat <- ifelse(preds > 0.5, "Burn", "No Burn")
confusionMatrix(preds_cat, geo_data$burn[-c(35,64,79)])

```

The summary output below reflects the final, suggested model for these data. 

```{r}
summary(model_geo)
```

#### Please note:
*The interpretation of the estimated coefficients needs to consider the logistic nature of these estimating procedures. Please don't hesitate to ask for help*

# References
